{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917e0ac3-30be-4143-a5aa-82aeed3e52aa",
   "metadata": {},
   "source": [
    "# Interactive version of `scripts/text2img.py`\n",
    "\n",
    "This notebook was tested using Jupyter Lab.\n",
    "\n",
    "Uses Jupyter widgets (not Google Colab form fields) for compatibility on both Jupyter Lab and Colab, see: https://colab.research.google.com/notebooks/forms.ipynb.\n",
    "\n",
    "To install widgets for Jupyter Lab, follow the instructions here: https://ipywidgets.readthedocs.io/en/latest/user_install.html#installing-in-jupyterlab-3-0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5096dd0-3ed1-4f04-a468-58c7bf5d2205",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup code (run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830741bb-4e5b-4d8f-93e8-affd73909f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME hack to get code in txt2img (run from base project directory) to work in a notebook located inside /scripts\n",
    "import os\n",
    "\n",
    "if os.path.basename(os.getcwd()) == 'scripts':\n",
    "    # doesn't check that we're actually in the root directory, hence the hack\n",
    "    os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689b0594-4c5b-43b0-8540-9d5d38cf1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly modified version of: https://github.com/CompVis/stable-diffusion/blob/main/scripts/txt2img.py\n",
    "\n",
    "import argparse, os, sys, glob    \n",
    "import torch    \n",
    "import numpy as np    \n",
    "from omegaconf import OmegaConf    \n",
    "from PIL import Image    \n",
    "from tqdm.notebook import tqdm, trange  # NOTE: updated for notebook\n",
    "from itertools import islice    \n",
    "from einops import rearrange    \n",
    "from torchvision.utils import make_grid    \n",
    "import time    \n",
    "from pytorch_lightning import seed_everything    \n",
    "from torch import autocast    \n",
    "from contextlib import contextmanager, nullcontext    \n",
    "    \n",
    "from ldm.util import instantiate_from_config    \n",
    "from ldm.models.diffusion.ddim import DDIMSampler    \n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from scripts.txt2img import chunk, load_model_from_config\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def load_model(opt):\n",
    "    \"\"\"Seperates the loading of the model from the inference\"\"\"\n",
    "    \n",
    "    if opt.laion400m:\n",
    "        print(\"Falling back to LAION 400M model...\")\n",
    "        opt.config = \"configs/latent-diffusion/txt2img-1p4B-eval.yaml\"\n",
    "        opt.ckpt = \"models/ldm/text2img-large/model.ckpt\"\n",
    "        opt.outdir = \"outputs/txt2img-samples-laion400m\"\n",
    "\n",
    "    config = OmegaConf.load(f\"{opt.config}\")\n",
    "    model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def run_inference(opt, model):\n",
    "    \"\"\"Seperates the loading of the model from the inference\n",
    "    \n",
    "    Additionally, slightly modified to display generated images inline\n",
    "    \"\"\"\n",
    "    seed_everything(opt.seed)\n",
    "\n",
    "    if opt.plms:\n",
    "        sampler = PLMSSampler(model)\n",
    "    else:\n",
    "        sampler = DDIMSampler(model)\n",
    "\n",
    "    os.makedirs(opt.outdir, exist_ok=True)\n",
    "    outpath = opt.outdir\n",
    "\n",
    "    batch_size = opt.n_samples\n",
    "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "    if not opt.from_file:\n",
    "        prompt = opt.prompt\n",
    "        assert prompt is not None\n",
    "        data = [batch_size * [prompt]]\n",
    "\n",
    "    else:\n",
    "        print(f\"reading prompts from {opt.from_file}\")\n",
    "        with open(opt.from_file, \"r\") as f:\n",
    "            data = f.read().splitlines()\n",
    "            data = list(chunk(data, batch_size))\n",
    "\n",
    "    sample_path = os.path.join(outpath, \"samples\")\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "    grid_count = len(os.listdir(outpath)) - 1\n",
    "\n",
    "    start_code = None\n",
    "    if opt.fixed_code:\n",
    "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "\n",
    "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                tic = time.time()\n",
    "                all_samples = list()\n",
    "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "                    for prompts in tqdm(data, desc=\"data\"):\n",
    "                        uc = None\n",
    "                        if opt.scale != 1.0:\n",
    "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                        if isinstance(prompts, tuple):\n",
    "                            prompts = list(prompts)\n",
    "                        c = model.get_learned_conditioning(prompts)\n",
    "                        shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                        samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
    "                                                         conditioning=c,\n",
    "                                                         batch_size=opt.n_samples,\n",
    "                                                         shape=shape,\n",
    "                                                         verbose=False,\n",
    "                                                         unconditional_guidance_scale=opt.scale,\n",
    "                                                         unconditional_conditioning=uc,\n",
    "                                                         eta=opt.ddim_eta,\n",
    "                                                         x_T=start_code)\n",
    "\n",
    "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "                        if not opt.skip_save:\n",
    "                            for x_sample in x_samples_ddim:\n",
    "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                                Image.fromarray(x_sample.astype(np.uint8)).save(\n",
    "                                    os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                                base_count += 1\n",
    "\n",
    "                        if not opt.skip_grid:\n",
    "                            all_samples.append(x_samples_ddim)\n",
    "\n",
    "                if not opt.skip_grid:\n",
    "                    # additionally, save as grid\n",
    "                    grid = torch.stack(all_samples, 0)\n",
    "                    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "                    grid = make_grid(grid, nrow=n_rows)\n",
    "\n",
    "                    # to image\n",
    "                    grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "                    Image.fromarray(grid.astype(np.uint8)).save(os.path.join(outpath, f'grid-{grid_count:04}.png'))\n",
    "                    grid_count += 1\n",
    "                    \n",
    "                    # display\n",
    "                    if opt.display_inline:\n",
    "                        clear_output()\n",
    "                        display(Image.fromarray(grid.astype(np.uint8)))\n",
    "\n",
    "                toc = time.time()\n",
    "\n",
    "    print(f\"Your samples have been saved to: \\n{outpath} \\n\"\n",
    "          f\" \\nEnjoy.\")\n",
    "\n",
    "\n",
    "def run(opt):\n",
    "    \"\"\"If the model parameters changed, reload the model, otherwise, just do inference\"\"\"\n",
    "\n",
    "    print(f\"Creating image ({opt.H},{opt.W}) from prompt:\\n\\\"{opt.prompt}\\\"\\n\")\n",
    "    \n",
    "    # FIXME global hack\n",
    "    global last_config\n",
    "    global last_ckpt\n",
    "    global model\n",
    "\n",
    "    if (opt.config != last_config) or (opt.ckpt != last_ckpt):\n",
    "        model = load_model(opt)\n",
    "        # FIXME global hack\n",
    "        last_config = opt.config\n",
    "        last_ckpt   = opt.ckpt\n",
    "\n",
    "    run_inference(opt, model)\n",
    "\n",
    "\n",
    "# FIXME global hack\n",
    "last_config = \"\"\n",
    "last_ckpt   = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80733312-875a-4e24-adac-9eb884c1666b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to turn kwargs into Jupyter widgets\n",
    "import ipywidgets as widgets\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def get_widget_extractor(widget_dict):\n",
    "    # allows accessing after setting, this is to reduce the diff against the argparse code\n",
    "    class WidgetDict(OrderedDict):\n",
    "        def __getattr__(self,val):\n",
    "            return self[val].value\n",
    "    return WidgetDict(widget_dict)\n",
    "\n",
    "\n",
    "# Allows long widget descriptions\n",
    "style  = {'description_width': 'initial'}\n",
    "# Force widget width to max\n",
    "layout = widgets.Layout(width='100%')\n",
    "\n",
    "# args from argparse converted to widgets:\n",
    "# https://github.com/CompVis/stable-diffusion/blob/main/scripts/txt2img.py#L48-L177\n",
    "widget_opt = OrderedDict()\n",
    "widget_opt['prompt'] = widgets.Text(\n",
    "    layout=layout, style=style,\n",
    "    description='the prompt to render',\n",
    "    #value=\"a painting of a virus monster playing guitar\",  # script default\n",
    "    value=\"a photograph of an astronaut riding a horse\",  # README default\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['outdir'] = widgets.Text(\n",
    "    layout=layout, style=style,\n",
    "    description='dir to write results to',\n",
    "    value=\"outputs/txt2img-samples\",\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['skip_grid'] = widgets.Checkbox(\n",
    "    layout=layout, style=style,\n",
    "    value=False,\n",
    "    description='do not save a grid, only individual samples. Helpful when evaluating lots of samples',\n",
    "    indent=False,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['skip_save'] = widgets.Checkbox(\n",
    "    layout=layout, style=style,\n",
    "    value=False,\n",
    "    description='do not save individual samples. For speed measurements.',\n",
    "    indent=False,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['ddim_steps'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='number of ddim sampling steps',\n",
    "    value=50,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['plms'] = widgets.Checkbox(\n",
    "    layout=layout, style=style,\n",
    "    value=True,\n",
    "    description='use plms sampling',\n",
    "    indent=False,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['laion400m'] = widgets.Checkbox(\n",
    "    layout=layout, style=style,\n",
    "    value=False,\n",
    "    description='uses the LAION400M model',\n",
    "    indent=False,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['fixed_code'] = widgets.Checkbox(\n",
    "    layout=layout, style=style,\n",
    "    value=False,\n",
    "    description='if enabled, uses the same starting code across samples',\n",
    "    indent=False,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['ddim_eta'] = widgets.FloatText(\n",
    "    layout=layout, style=style,\n",
    "    description='ddim eta (eta=0.0 corresponds to deterministic sampling',\n",
    "    value=0.0,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['n_iter'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='sample this often',\n",
    "    value=2,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['H'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='image height, in pixel space',\n",
    "    value=512,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['W'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='image width, in pixel space',\n",
    "    value=512,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['C'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='latent channels',\n",
    "    value=4,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['f'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='downsampling factor',\n",
    "    value=8,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['n_samples'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='how many samples to produce for each given prompt. A.k.a. batch size',\n",
    "    value=3,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['n_rows'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='rows in the grid (default: n_samples)',\n",
    "    value=0,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['scale'] = widgets.FloatText(\n",
    "    layout=layout, style=style,\n",
    "    description='unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))',\n",
    "    value=7.5,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['from_file'] = widgets.Text(\n",
    "    layout=layout, style=style,\n",
    "    description='if specified, load prompts from this file',\n",
    "    value=None,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['config'] = widgets.Text(\n",
    "    layout=layout, style=style,\n",
    "    description='path to config which constructs model',\n",
    "    value=\"configs/stable-diffusion/v1-inference.yaml\",\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['ckpt'] = widgets.Text(\n",
    "    layout=layout, style=style,\n",
    "    description='path to checkpoint of model',\n",
    "    value=\"models/ldm/stable-diffusion-v1/model.ckpt\",\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['seed'] = widgets.IntText(\n",
    "    layout=layout, style=style,\n",
    "    description='the seed (for reproducible sampling)',\n",
    "    value=42,\n",
    "    disabled=False\n",
    ")\n",
    "widget_opt['precision'] = widgets.Combobox(\n",
    "    layout=layout, style=style,\n",
    "    description='evaluate at this precision',\n",
    "    value=\"autocast\",\n",
    "    options=[\"full\", \"autocast\"],\n",
    "    disabled=False\n",
    ")\n",
    "# Extra option for the notebook\n",
    "widget_opt['display_inline'] = widgets.Checkbox(\n",
    "    layout=layout, style=style,\n",
    "    value=True,\n",
    "    description='display output images inline (in addition to saving them)',\n",
    "    indent=False,\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Button that runs the \n",
    "# Alternatively, you can just run the following in a new cell:\n",
    "# run(get_widget_extractor(widget_opt))\n",
    "run_button = widgets.Button(\n",
    "    description='CLICK TO DREAM',\n",
    "    disabled=False,\n",
    "    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    tooltip='Click to run (settings will update automatically)',\n",
    "    icon='check'\n",
    ")\n",
    "run_button_out = widgets.Output()\n",
    "def on_run_button_click(b):\n",
    "    with run_button_out:\n",
    "        clear_output()\n",
    "        run(get_widget_extractor(widget_opt))\n",
    "run_button.on_click(on_run_button_click)\n",
    "\n",
    "# Package into box and render\n",
    "#primary_options = ['prompt', 'outdir']  # options to put up top\n",
    "#secondary_options = [k for k in widget_opt.keys() if k not in primary_options]  # rest, ordered by insertion\n",
    "\n",
    "load_options = ['config', 'ckpt']\n",
    "inference_options = [k for k in widget_opt.keys() if k not in load_options]  # rest, ordered by insertion\n",
    "assert all([k in inference_options + load_options for k in widget_opt.keys()])  # make sure we didn't miss any options\n",
    "\n",
    "# Package into box for rendering\n",
    "gui = widgets.VBox(\n",
    "    [widget_opt[k] for k in inference_options] + [widget_opt[k] for k in load_options] + [run_button, run_button_out]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0cc2d-1945-4dad-ae64-d4878edc5c38",
   "metadata": {},
   "source": [
    "## Interactive loop\n",
    "\n",
    "Change options using the GUI, then run the next cell - no need to re-run/display the GUI cell (the GUI will automatically update the variables)\n",
    "\n",
    "You may get a warning (eg \"Some weights of the model ...\") the first time you run the cell when the model is first loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b07a2631-a9b5-432d-a257-0710a7841e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b27342185ff4175a89a15455f06d215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='a photograph of an astronaut riding a horse', description='the prompt to render', lâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(gui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24efa22-6ea3-45b0-ae1f-493079b3eff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm2",
   "language": "python",
   "name": "ldm2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
